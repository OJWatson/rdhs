##' Create a data frame of datasets that your log in can download
##'
##' DHS datasets that can be downloaded
##' @param your_email Character for email address for DHS website
##' @param your_password Character for password for DHS website
##' @param your_project Character string for the name of your project that gives you access to the DHS database
##' @param datasets_api_results Data.table for the api results for the datasets endpoint. Default = NULL and
##' generated by default if not declared.
##' @param surveys_api_results  Data.table for the api results for the surveys endpoint. Default = NULL and
##' generated by default if not declared.
##'
##' @note Credit for function to \url{https://github.com/ajdamico/lodown/blob/master/R/dhs.R}
##'
##' @return Returns \code{"data.frame"} of length 14:
##' \itemize{
##'       \item{"FileFormat"}
##'       \item{"FileSize"}
##'       \item{"DatasetType"}
##'       \item{"SurveyNum"}
##'       \item{"SurveyId"}
##'       \item{"FileType"}
##'       \item{"FileDateLastModified"}
##'       \item{"SurveyYearLabel"}
##'       \item{"SurveyType"}
##'       \item{"SurveyYear"}
##'       \item{"DHS_CountryCode"}
##'       \item{"FileName"}
##'       \item{"CountryName"}
##'       \item{"URLS"}
##'       }
##'
available_datasets <- function(your_email, your_password, your_project,
                               datasets_api_results = NULL,
                               surveys_api_results = NULL){


  # fetch all the datasets meta from the api if to already passed
  if(is.null(datasets_api_results) | is.null(surveys_api_results)){
    datasets_api_results <- dhs_datasets()
    surveys_api_results <- dhs_surveys()
  }

  # set up temp file for unpacking bins
  tf <- tempfile(fileext = ".txt")
  values <- authenticate_dhs( your_email , your_password , your_project )

  # grab project number here
  project_number <- values$proj_id

  # re-access the download-datasets page
  z <- httr::POST( "https://dhsprogram.com/data/dataset_admin/download-datasets.cfm" , body = list( proj_id = project_number ) )

  # write the information from the `countries` page to a local file
  writeBin( z$content , tf )

  # load the text
  y <- readLines( tf , warn = FALSE )

  # Create post request for the download manager
  values <- list(Proj_ID = project_number,
                 action = "downloadmanager")

  # Head to download page
  z <- httr::POST( "https://dhsprogram.com/data/dataset_admin/index.cfm" , body = values)

  # Grab the content from that and start creation for last post request
  writeBin( z$content , tf )
  # load the text
  y <- readLines( tf , warn = FALSE )


  # Donqwload manager post creation
  ctrycodelist_lines <- grep("name=\"ctrycodelist\" value=",y,value = TRUE)
  ctrycodelist <- qdapRegex::rm_between(ctrycodelist_lines, '"', '"', extract=TRUE) %>% lapply(function(x) x[3])
  names(ctrycodelist) <- rep("ctrycodelist",length(ctrycodelist))

  filedatatypelist_DHS_lines <- grep("name=\"filedatatypelist_",y,value = TRUE)
  filedatatypelist_DHS <- qdapRegex::rm_between(filedatatypelist_DHS_lines, '"', '"', extract=TRUE) %>% lapply(function(x) x[3])
  names(filedatatypelist_DHS) <- paste0("filedatatypelist_",qdapRegex::rm_between(filedatatypelist_DHS_lines,"filedatatypelist_","\" value",extract=TRUE))

  fformatlist <- grep("fformatlist",y,value = TRUE)
  fformatlist <- qdapRegex::rm_between(fformatlist, '"', '"', extract=TRUE) %>% lapply(function(x) x[3])
  names(fformatlist) <- rep("fformatlist",length(fformatlist))

  values <- list(surveymode = "all",
                 Proj_ID = project_number,
                 action = "downloadmanager",
                 subaction = "Build URL File List",
                 sub = "submit",
                 submit = "Build URL File List",
                 FileDataTypeCode = "",
                 ctrycode = "")

  values <- append(values,values = c(ctrycodelist,filedatatypelist_DHS,fformatlist))

  # submit request for all the possible datasets
  message("Creating Download url list from DHS website...")
  z <- httr::POST( "https://dhsprogram.com/data/dataset_admin/index.cfm" , body = values)
  link.urls <- xml2::xml_find_all(httr::content(z),"//a" )

  # pull all links download and read in
  url_link <- paste0("https://dhsprogram.com",grep(pattern = "/data/download/urlslist",
                                                   xml2::xml_attr(link.urls,"href"),value = TRUE))
  httr::GET(url_link , destfile = tf, httr::write_disk( tf , overwrite = TRUE ))
  urls <- readLines(tf)
  urls <- urls[-which(!nzchar(urls))]

  # start filling in the end result data frame of all available datasets
  res <- matrix(data = "",nrow = length(urls),ncol = dim(datasets_api_results)[2]+1)
  colnames(res) <- c(names(datasets_api_results),"URLS")
  res <- as.data.frame(res,stringsAsFactors = FALSE)
  res$URLS <- urls
  res$FileName <- qdapRegex::rm_between(urls,"Filename=","&Tp",extract = TRUE) %>% unlist
  res$DHS_CountryCode <- qdapRegex::rm_between(urls,"Ctry_Code=","&surv_id",extract = TRUE) %>% unlist

  # match meta using filenames and countrycodes (India has subnational datasets that clash)
  fileName_matches <- match(paste0(toupper(res$FileName),toupper(res$DHS_CountryCode)),
                            paste0(toupper(datasets_api_results$FileName),toupper(datasets_api_results$DHS_CountryCode)))
  res_matches <- which(!is.na(fileName_matches))
  if(sum(is.na(fileName_matches))>0) fileName_matches <- fileName_matches[-which(is.na(fileName_matches))]
  res[res_matches,1:length(datasets_api_results)] <- datasets_api_results[fileName_matches,]

  # if this is greater than 0, then their API is liekly out of date (which is an issue...)
  # TODO: Ask about how they would like to be contacated about this
  missings <- which(res$FileFormat=="")
  if(length(missings)>0){

    missing_country_codes <- unique(res$FileName[missings] %>% substr(start = 1, stop = 2))
    call <- surveys_api_results
    res$SurveyNum[missings] <- (qdapRegex::rm_between(res$URLS[missings],"surv_id=","&dm",extract = TRUE) %>% unlist)
    missing_survey_nums <- unique(res$SurveyNum[missings])

    ## TODO: MAKE THIS NOT TERRIBLE - will most likely break, and there is no telling that the surveys api is any better
    needed_cols <- c("SurveyId","SurveyYearLabel","SurveyType","SurveyYear","DHS_CountryCode","CountryName")

    # grab file types and format types for matching from the filenames
    file_types <- unique(res$FileType)
    file_types <- file_types[-which(file_types=="")]
    filetype_stems <- lapply(file_types,function(x) unique(toupper(substr(res$FileName[res$FileType==x],3,4)))) %>% unlist

    file_formats <- unique(res$FileFormat)
    file_formats <- file_formats[-which(file_formats=="")]
    fileformat_endings <- lapply(file_formats,function(x) unique(toupper(substr(res$FileName[res$FileFormat==x],7,8)))) %>% unlist

    # loop through all the missing survery numbers that we need to fill in for
    for(i in missing_survey_nums){

      # where are these missing datasets
      missing_pos <- missings[which(res$SurveyNum[missings]==i)]

      # is there any info about them from the surveys api
      if(length(which(call$SurveyNum==i))>0){
        res[missing_pos,needed_cols] <- call[which(call$SurveyNum==i),needed_cols]
      }

      # fill in file type and format from filename
      res$FileType[missing_pos] <- file_types[match(toupper(substr(res$FileName[missing_pos],3,4)),filetype_stems)]
      res$FileFormat[missing_pos] <- file_formats[match(toupper(substr(res$FileName[missing_pos],3,4)),fileformat_endings)]
    }

  }

  return(res)

}


##' Create a data frame of datasets that your log in can download
##'
##' Download datasets specified using output of \code{available_datasets}.
##' @param desired_dataset Row from \code{available_datasets}
##' @param download_option Chracter dictating how the durvey is stored when downloaded. Must be one of:
##' \itemize{
##'       \item{"zip"} - Just the zip. "z" or   will match
##'       \item{"rds"} - Just the read in and saved rds. "r" or anything like will match
##'       \item{"both"} - Both the rds and extract. "b" or anything like will match
##'}
##' @param reformat Boolean detailing whether dataset rds should be reformatted for ease of use later. Default = TRUE
##' @param all_lower Logical indicating whether all value labels should be lower case. Default to `TRUE`.
##' @param output_dir_root Directory where files are to be downloaded to
##' @param your_email Character for email address for DHS website
##' @param your_password Character for password for DHS website
##' @param your_project Character string for the name of your project that gives you access to the DHS database
##' @param ... Any other arguments to be passed to \code{\link{read_dhs_dataset}}
##'
download_datasets <- function(desired_dataset,
                              download_option = "both",
                              reformat=TRUE,
                              all_lower=TRUE,
                              output_dir_root=NULL,
                              your_email , your_password , your_project,
                              ...){

  # possible download options:
  download_possibilities <- c("zip","rds","both")
  download_option <- grep(paste0(strsplit(download_option,"") %>% unlist,collapse="|"),download_possibilities)
  if(!is.element(download_option,1:3)) stop("Download option specified not one of zip,rds,both")

  # handle output dir
  dataset_dir <- file.path(output_dir_root)
  if(reformat){
    dataset_dir <- paste0(dataset_dir,"_reformatted")
  }

  # make sure the folder exists and create the zip path
  dir.create( dataset_dir , showWarnings = FALSE, recursive = T )
  zip_path <- file.path(dataset_dir,desired_dataset$file)

  # login
  values <- authenticate_dhs( your_email , your_password , your_project )
  project_number <- values$proj_id

  # access the download-datasets page
  z <- httr::POST( "https://dhsprogram.com/data/dataset_admin/download-datasets.cfm" ,
                   body = list( proj_id = project_number ) )

  # download our zip and parse the response for any errors
  message("Downloading: \n", paste(desired_dataset$CountryName,desired_dataset$SurveyYear,
                                   desired_dataset$SurveyType,desired_dataset$FileType,
                                   desired_dataset$FileFormat, collapse=", "))

  # set up temp file for unpacking bins
  # annoyingly we have to do this because some zips have been zipped with the same name several times
  # so they can not be unzipped to the same directory. Thus we bounce unzips between these two dirs.
  tf <- tempfile()
  tdir1 <- tempfile()
  tdir2 <- tempfile()
  on.exit(unlink(c(tf,tdir1,tdir2),recursive = TRUE,force=TRUE))

  # download zip to our tempfile
  resp <- httr::GET(desired_dataset$URLS[1], destfile = tf,
                    httr::write_disk( tf , overwrite = TRUE ),
                    httr::progress()) %>% handle_api_response(to_json=FALSE)


  # check that the zip doesn't contain nested zips and if does keep extracting till the correct zip is found
  # this is slightly ugly but works
  unzipped_files <- unzip_warn_fails(tf , list=TRUE)
  unzip_round <- 1
  while(sum(!is.na(grep("\\.zip",unzipped_files$Name,ignore.case = TRUE)))>0){
    if(unzip_round==1){
      unzipped_files <- unzip_warn_fails(tf,exdir = tdir1,overwrite=TRUE)
      unzip_round <- 2
    } else {
      unzipped_files <- unzip_warn_fails(tf,exdir = tdir2,overwrite=TRUE)
      unzip_round <- 1
    }
    unzipped <- unzipped_files[grep(desired_dataset$FileName,unzipped_files,ignore.case = TRUE)]
    unzipped_files <- unzip_warn_fails(unzipped , list=TRUE)
    tf <- unzipped
    if(unzip_round==1) {
      suppressWarnings(file.remove(list.files(tdir1,full.names = TRUE)))
    } else {
      suppressWarnings(file.remove(list.files(tdir2,full.names = TRUE)))
    }
  }

  ## DOWNLOAD OPTIONS HANDLING:

  # 1. Just the zip - we'll always do this and then if it's 2 remove it later

  # if it's just the zip then we copy it to the directory return the file path for this
  res <- file.copy(tf,to = zip_path,overwrite = TRUE)
  res <- if(res) zip_path else stop("Failed to donwload zip to where client root is - check write access?")


  # 2/3. rds or both
  if(download_option>=2){

    # now read the dataset in with the requested reformat options
    res <- read_dhs_dataset(zip_path,reformat,all_lower,...)

    # handle results. If it's character it's because we haven't yet got a parser we are happy with
    if(!is.character(res)){

      # let's assign the file name attribute to the res
      attr(res$dataset,which = "filename") <- desired_dataset$file

      # set up the rds_path to save the dataset
      rds_path <- file.path(dataset_dir,paste0(desired_dataset$file,".rds"))

      # if we reformatted return the dataset with the description table for ease of reference
      if(reformat){
        saveRDS(res,rds_path)
      } else {
        saveRDS(res$dataset,rds_path)
      }


      # if the class of the object is from a geo file then we will just return the rds path
      if(class(res$dataset)[1]=="SpatialPointsDataFrame"){
        res <- rds_path
      } else {
        # if its a dataset then we return the path and the code_descriptions as these are useful to have cached
        res$dataset <- rds_path
      }
    }

    # 3. If not both then delete the zip
    if(download_option!=3){
      file.remove(zip_path)
    }

  }

  message( "Dataset download finished" )
  return(res)

}



##' Autheticate Users for DHS website
##'
##' @title DHS Wesbite Authentication
##' @param your_email Character for email address for DHS website
##' @param your_password Character for password for DHS website
##' @param your_project Character string for the name of your
##' project that gives you access to the DHS database
##'
##'
##' @note Credit for function to \url{https://github.com/ajdamico/lodown/blob/master/R/dhs.R}
##'
##' @return Returns list of length 3:
##' \itemize{
##'       \item{"user_name"}{ your email usually}
##'       \item{"user_pass"}{ your pasword you provided}
##'       \item{"proj_id"}{ your project number that will enable your project to be accessed downstream}
##'       }
##'
##'
##'

authenticate_dhs <- function( your_email , your_password , your_project ){


  # Argument Checking
  if(!is.character(your_email)) stop ("your_email is not a character string")
  if(!is.character(your_project)) stop ("your_password is not a character string")
  if(!is.character(your_password)) stop ("your_project is not a character string")

  # authentication page
  terms <- "https://dhsprogram.com/data/dataset_admin/login_main.cfm"

  # countries page
  downloads_page <- "https://dhsprogram.com/data/dataset_admin/download-datasets.cfm"

  # create a temporary file
  tf <- tempfile(fileext = ".txt")

  # set the username and password
  values <-
    list(
      UserName = your_email ,
      UserPass = your_password ,
      Submitted = 1 ,
      UserType = 2
    )

  # log in.
  message("Logging into DHS website...")
  #httr::GET( terms , query = values )
  z <- httr::POST( terms , body = values)

  # extract the available countries from the projects page
  #z <- httr::GET( downloads )

  # write the information from the `projects` page to a local file
  writeBin( z$content , tf )

  # load the text
  y <- readLines( tf , warn = FALSE )

  # figure out the project number - only use first 30 chars due to ellipsis formation
  project.line <- unique( y[ grepl( "option value" , y ) &
                               grepl( paste0(strsplit(your_project,"")[[1]][1:30],collapse="") , y , fixed = TRUE ) ] )

  # confirm only one project
  stopifnot( length( project.line ) == 1 )

  # extract the project number from the line above
  project_number <- gsub( "(.*)<option value=\"([0-9]*)\">(.*)" , "\\2" , project.line )

  # remove the tf
  suppressWarnings( file.remove( tf ) )

  # log in again, but specifically with the project number
  res <- list(
    user_name = your_email ,
    user_pass = your_password ,
    proj_id = project_number
  )



}
