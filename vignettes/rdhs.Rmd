---
title: "rdhs overview"
author: "OJ Watson"
date: '`r Sys.Date()`'
output:
  rmarkdown::html_vignette:
    toc: true
    keep_md: true
vignette: >
  %\VignetteIndexEntry{Overview}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE}
NOT_CRAN <- identical(tolower(Sys.getenv("NOT_CRAN")), "true")
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  purl = NOT_CRAN,
  eval = NOT_CRAN,
  error = TRUE
)
```

## Overview

The rdhs package helps users interact with the DHS API, as well as facilitate downloading 
DHS survey datasets and extract individual level data from those surveys. This process
is described below and should cover most functionality that will be needed for working
with these datasets. 

## rdhs client

rdhs was built around an api client that would handle making and parsing api requests
to the DHS api. From that, a more nuanced general client tool was created that both made
api requests, ensuring that results are cached so they are not looked up again, but also
handled the downstream functionality of rdhs. As a result the best way to demnstrate rdhs
is to consider the current pipeline through which you might first use rdhs. Each of the following
steps are all carried out using the rdhs client, which is an R6 class (similar to R's built in
reference classes and make caching survey and api queries more reproducible):

1. Create the rdhs client
2. Making api requests
3. Using api requests to identify surveys you want to download
4. Cataloguing which surveys you can download for your log in credentials
5. Downloading the desired surveys from step 3 from the available surveys from step 4
6. Creating the list of survey questions required for your investigation
7. Using the list from step 6 to create your data frame of survey responses
8. Storing your pipeline from step 1 - 7 to increase reproducibility

Before demonstrating each section, it is worth highlighting why all the above steps are
carried out using the highly object-orientated client. The client itself caches the results
from each of the above step locally in one way or another. For example, the client when first
created is saved in a new local directory that you have specified. Once created it stores the 
time at which it was last updated, so that everytime you use rdhs afterwards, the client will
check to see if any of the surveys you have downloaded, or api calls you have made are now out
of date. If this happens, the client will let you know, before giving you options over whether
to a) compleletely delete the client's cache, or b) update any cached surveys or information
in bulk now or c) as and when you itneract with out of date information. In this way, 
rdhs hopes to allow studies and analayis of DHS data to both be easier, and reproducible allowing
researchers to save their analysis pipeline so that others can replicate it easily.

---

## 0. Installing the package

```{r Installing the package, include=TRUE, message = FALSE, warning = FALSE}

# First make sure the package is installed
devtools::install_github("OJWatson/rdhs")

# Load package
library(rdhs)
```

## 1. Create the rdhs client

The client is the bulk of the package's functionality, and at the moment there are
3 named arguments, *api_key*, *root* and *credentials*. You can apply for an API key from the DHS
website that will allow you to leverage more api call returns, and will be stroed within
the client if declared here. The root argument is the directory path where the client
and associated caches will be stored. If left bank, a suitable directory will be created
within your user cahce directory for your operating system. E.g:

* Mac OS X: ‘~/Library/Caches/<AppName>’
* Unix: ‘~/.cache/<AppName>’, $XDG_CACHE_HOME if defined

The credential argument is your login information for the DHS website. THis should take the form
of a file path that contains 3 lines, something like this:

email="dummy@gmail.com"
password="dummypass"
project="Dummy Project"

These will then be read in and used to create system environment variables so you won't have to 
re-enter them within the same session. These will be set to rdhs_USER_PASS/EMAIL/PROJECT, and will be
first looked for when trying to create a client. If a credentials argument is provided, this will be used
first though.

```{r Client creation, include=TRUE, message = FALSE, warning = FALSE, purl = NOT_CRAN, eval = NOT_CRAN}

# Create a client withi our api key and in this root directory
root <- file.path(getwd(),"rdhs_vignette_client_root")

# This if shows that you can make a client if  
if(file.exists("credentials")){
  client <- rdhs::dhs_client(api_key = "ICLSPH-527168",credentials = "credentials",root = root)
} else {
  client <- rdhs::dhs_client(api_key = "ICLSPH-527168",root = root)
}

# Have a look at what the client object is
client

```

The root directory you chose will now also have in it an .rds file that is your dhs client, 
and a series of subfolders that are used to cache data. In order to use this cached data when 
you start your next R session you would use the same code as above, but this time it will 
know you have already created a client and will load it instead. 

## 2. Making api requests

The client can then be used to conduct api requests. To find more information about the
possible api requests head to their [website](https://api.dhsprogram.com/#/index.html).
In brief the api allows for a number of different endpoints to be accessed that constitute one of
12 different databases. These are stored within the client by default, and are the following:

* [data](https://api.dhsprogram.com/#/api-data.cfm)
* [indicators](https://api.dhsprogram.com/#/api-indicators.cfm)
* [countries](https://api.dhsprogram.com/#/api-countries.cfm)
* [surveys](https://api.dhsprogram.com/#/api-surveys.cfm)
* [surveycharacteristics](https://api.dhsprogram.com/#/api-surveycharacteristics.cfm)
* [publications](https://api.dhsprogram.com/#/api-publications.cfm)
* [datasets](https://api.dhsprogram.com/#/api-datasets.cfm)
* [geometry](https://api.dhsprogram.com/#/api-geometry.cfm)
* [tags](https://api.dhsprogram.com/#/api-tags.cfm)
* [dataupdates](https://api.dhsprogram.com/#/api-dataupdates.cfm)
* [uiupdates](https://api.dhsprogram.com/#/api-uiupdates.cfm)
* [info](https://api.dhsprogram.com/#/api-info.cfm)

These endpoints can then be further filtered, with a different set of filters possible for 
each endpoint. At current, these are not built in so it is possible to make human errors when
submitting filter queries. To find info about all the possible filter queries head to the associated
webpage from above.

For example we can now query to find out the trends in antimalarial use in Africa, and see if 
perhaps antimalarial perscription has decreased after RDTs were introduced (assumed 2010). So we can query 
for that by specifying the "data" endpoint, and using suitable filter queries provided as a list as follows:


```{r API call, include=TRUE, message = FALSE, warning = FALSE, fig.width=10, fig.height=8}

# Make an api request
resp <- client$dhs_api_request(api_endpoint = "data",
                       query = list("indicatorIds"="ML_FEVT_C_AML",
                                    "surveyYearStart"=2006,
                                    "breakdown"="subnational"))


# and let's filter it by some countries
countries  <- c("Angola","Ghana","Kenya","Liberia",
                "Madagascar","Mali","Malawi","Nigeria",
                "Rwanda","Sierra Leone","Senegal","Tanzania","Uganda")

library(ggplot2)
ggplot(resp[resp$CountryName %in% countries,],
       aes(x=SurveyYear,y=Value,colour=CountryName)) +
  geom_point() +
  geom_smooth(method = "glm") + 
  ylab(resp$Indicator[1]) + 
  facet_wrap(~CountryName)

```

If we miss entered a filter query (very possible) rdhs will give you an error that is
somewhat nicely presented on the pagee for you to see way:

```{r API fail, include=TRUE, message = FALSE, error=TRUE, purl= FALSE, warning = FALSE,cache=TRUE}

# Make an api request
resp <- client$dhs_api_request(api_endpoint = "data",
                       query = list("indicatorIds"="ML_FEVT_C_AMasfafasfL",
                                    "surveyYearStart"=202231231306,
                                    "breakdownAndCry"="subParTyping"))

```


rdhs by default converts the response to a data.frame and will ensure that all the api results
are returned. If you want to return fewer than all the results, or lok at the actual api response
then these can be controlled using the **num_results** and **just_results** arguments. 

Importantly the api request you have made has now been cached by the package. This means that
the next time you run this request, the result both returns quicker and you can access your data
even if not connected to the internet.

What if the DHS api is updated? Will my cached api calls now be wrong? Potentially. It is unclear
from the api how much information changes over time. The [dataupdates](https://api.dhsprogram.com/rest/dhs/dataupdates?f=html) 
endpoint shows updates to specific surveys. However, when looking at the [datasets](https://api.dhsprogram.com/rest/dhs/datasets?f=html) endpoint we can see that there are many survey files that have been presumably modified after their release. 
This may be just reuploading, or it could have changed the data and thus subsequent date we may get from api calls. 
As such, at this moment in time, if the date at which any of the DHS api was last updated as found
within the dataupdates endpoint is more recent than the cache_date within our client we will automatically
clear our api call cache. The api calls are relatively lightweight, so will not be a major hindrance
in the future. This can be seen below:

```{r API dating, include=TRUE, message = FALSE, warning = FALSE}

# Get the current cache date that the client was last created at
client$get_cache_date()

# Now let's set it back in time a year
client$set_cache_date(client$get_cache_date()-(365*24*60*60))
client$get_cache_date()

# And save this client 
client$save_client()

# Now let's imagine we were restarting our R session another time and wanted to load our client we were using before
client <- rdhs::dhs_client(api_key = "ICLSPH-527168",
                           root = root, 
                           credentials = "credentials")
client$get_cache_date()

```

## 3. Using api requests to identify surveys you want to download

Working with the api can allow you to get good access to course level data, however you may want access
to the raw datasets to compile individual level data. To do this you will need to decide what surveys you 
want for your survey and then download them. This used to be laborious... and is somewhat nice now.

First we will have a look using the api what surveys record data about children under age five with fever 
in the two weeks preceding the survey that received any anti-malarial drugs who took Quinine. 

```{r API for surveys, include=TRUE, message = FALSE, warning = FALSE}

# Make an api request
desired_data <- client$dhs_api_request(api_endpoint = "surveys",
                       query = list("indicatorIds"="ML_AMLD_C_QNN"))

str(desired_data)

```

Now we have a data frame for the surveys we want we then go about actually downloading
the surveys.

## 4. Cataloguing which surveys you can download for your log in credentials

To do this we again use the client, and pass in our login in credentials, and where we will
eventually want to download our surveys too. The client will then log into your DHS account
for you and scrape what surveys you have access to. It will then turn these into downloadable
urls to be used in the next step. (Takes ~30s)


```{r Avaialable surveys, include=TRUE, message = FALSE, warning = FALSE}

# client will fetch all the surveys that you can download from the DHS 
# website (i.e. the ones they have gien you permission for)
available_surveys <- client$available_surveys()

str(available_surveys)
```


## 5. Downloading the desired surveys from step 3 from the available surveys from step 4

Now that we have the list of surveys, and we now what ones we want to actually download, we simply
subset those from step 4 using our dataframe in step 3.

```{r Subset surveys, include=TRUE, message = FALSE, warning = FALSE}

# subset our available data frames
subsetted_surveys <- dplyr::filter(available_surveys, SurveyId %in% desired_data$SurveyId)

```

And we can now download those surveys using our client.

N.B It is recommended (and there is only support for) to download the Stata datasets and the SPSS datasets,
and preferentially the Stata datasets as these are a little quicker to import. This is just the 
survey datasets, whereas the GPS coordinates (which are logged as .dat files) are supported as they are 
read in using \code{rgdal::readOGR}.

The .sav datasets are read in using \code{haven::read_sav()}, which is very good at bringing out all the factor
attributes. At the moment, .dta datasets are read in using \code{foreign::read_dta}. There is a small reason for not using
the `haven` pacakges .dta read in function, which is that it is marginally slower because it extracts all the attributes.
I left this thus as an option to compare the two packages, and give people a quicker download option. What will happen
in the future though is for both options to be specified for both datasets, as the extra attributes extracted using `haven` 
is really excellent for leaving no extracted data later to be open to missinterpretation.


```{r Download surveys, include=TRUE, message = FALSE, warning = FALSE, collapse = TRUE}

# let's then grab the stata datasets
stata_surveys <- subsetted_surveys[subsetted_surveys$FileFormat=="Stata dataset (.dta)",]

# download datasets
downloads <- client$download_survey(desired_survey=stata_surveys[c(1,25,50,75,100),], reformat = TRUE)

```

At the moment this will indisciriminately download the zip file and unpack it to a directory within
the directory path we specified as the output_dir argument for `available_surveys`. So this looks something like this:

```{r Download tree, include=TRUE, message = FALSE, warning = FALSE}

x <- lapply(strsplit(list.files(file.path(root,"surveys"),recursive = T,full.names = TRUE), "/"),
            function(z) as.data.frame(t(z)))
x <- dplyr::rbind_all(x)
x$pathString <- apply(x, 1, function(x) paste(trimws(na.omit(tail(x,4))), collapse="/"))
mytree <- data.tree::as.Node(x)
mytree

```

Where you can see that a new directory is made for each survey, with any datasets collected from that
survey, e.g. birth recode, individual recode, sotred within that same directory. 

Now to have a quick look at what was actually downloaded using the download_survey function.

```{r Explore filenames, include=TRUE, message = FALSE, warning = FALSE}
str(downloads)
```

What we have is the file paths to where the files were saved to. Depending on the download option
this will either be the zip file, the extracted files from the zip, or the rds object created by unzipping
the zip file and rading in the approporiate dataset. 

A note on the rds object. These will be automatically reformatted to remove factors and simply return the raw character representations
of the data. This is carried out using \code{rdhs:::dta_factor_format} and \code{rdhs:::haven_factor_format}, that will read the stata and
spss dataset attributes and subsequently correct factors. This will also return a table detailing what the dataset codes were for that survey and
what their description is. This will be very helpful in the next section when we aim to find all the possible questions we may want to query
our surveys by. The rationale behind this was simply that the factor levels will often not be consistent between surveys, and by carrying this 
out in the backend when these datasets are read in then it saves the user from having to look up factor levels later on. If you do not wish to have
this information then change the `reformat` argument to `FALSE`.

A quick look at a demonstration of the reformat:

```{r Explore downloads, include=TRUE, message = FALSE, warning = FALSE}

# let's read one result in and look at that
downloaded_dataset <- readRDS(downloads[[1]][1])
str(downloaded_dataset,list.len = 10)

# have a look at the survey code descriptions
head(downloaded_dataset$Survey_Code_Descrptions)

# and let's have a look at how the factor reformat has been helpful
downloaded_dataset$Survey_Code_Descrptions[downloaded_dataset$Survey_Code_Descrptions$Code=="ML101",]
head(downloaded_dataset$Survey$ML101,20)

```



## 6. Creating the list of survey questions required for your investigation

Now that we have all the surveys downloaded that we want, (which we worked out using the DHS API indicators),
we no want to know which survey codes/questions are contained in each survey that we want in our final extracted
data frame. To do this we continue using the client, which can iterate though our survey code descriptions.

The reason this is useful, is that some surveys have different codes for the same questions. Therefore it can be difficult
to filter by just choosing the codes you want from one survey. Similarly, the questions for the same code may be subtley
different, e.g. some report on say fever in the last 2 weeks and some report on fever in the last 4 weeks. So by querying
for terms, we can then use the survey specific codes that interest us, and we can use the descriptions to save information
about the questions easily. 

So let's now have a look at the surveys we donwloaded, and query for survey questions that included anything about fevers,
malaria, or tests. 

(At the moment the search terms are just combined to produce and OR regex query. )

```{r Survey questions, include=TRUE, message = FALSE, warning = FALSE}

# extract the questions, and filter by search terms.
questions <- client$survey_questions(stata_surveys[c(1,25,50,75,100),],search_terms = c("fever","malaria","test"))

# which will return something that shows all the valid survey questons
head(questions)
```

This resultant data frame can then be used to check which codes you may want to include to extract your raw data. 

## 7. Using the list from step 6 to create your data frame of survey responses

We can now quickly view and make a note of the codes that may be of interest to us, and either subset the
questions data.frame as needed or directly pass this on to extract the releve

## 8. Storing your pipeline from step 1 - 7 to increase reproducibility


## Summary and further thoughts

Hopefully the above tutorial has shown how the *rdhs* package can facilitate both querying the DHS
api and hopefully life some of the difficulty of interacting with the raw datasets. Any suggestions or 
comments/corrections/errors/ideas please let me know either in the issues or send me an email at 
"o.watson15@imperial.ac.uk".

---


